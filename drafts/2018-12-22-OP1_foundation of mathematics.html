---
layout: post
title:  "最优化算法(1):数学基础"
date:   2018-12-21 10:47:00
categories: Mathematics Optimization
excerpt: "从某种程度上说，我们生活中遇到的许许多多的问题，其本质都是一个优化问题。例如着装打扮、选择饭店、租购房屋、旅行规划等等。如果我们能将这些问题转化为目前数学上可解的最优化模型，并且我们掌握了求解相关最优化模型的最优化算法，那么我们或许能够生活得更聪明，更舒适，也更幸福。将生活问题转化为数学模型并不容易，它或许需要敏锐的头脑，需要长期的积累，需要灵光的乍泄，但掌握求解相关最优化模型的算法则相对容易多了"
permalink: /optimization/1/foundation_of_mathematics/
---

<div class="post-style">

<p>从某种程度上说，我们生活中遇到的许许多多的问题，都可以看成是一个最优化问题。例如着装打扮、选择饭店、租购房屋、旅行规划等等。如果我们能将这些问题转化为目前数学上可解的最优化模型，并且我们掌握了求解相关最优化模型的最优化算法，那么我们或许能够生活得更聪明，更舒适，也更幸福。将生活问题转化为数学模型并不容易，它或许需要敏锐的头脑，需要长期的积累，需要灵光的乍泄，但掌握求解相关最优化模型的算法则相对容易多了。</p>

<p><strong>1947 年，Dantzig 提出求解一般线性规划问题的单纯形法后，最优化开始成为一门独立的学科</strong>。历经 70 多年的风雨，在电子计算机的推动下，最优化理论与算法如今已在经济计划、工程设计、生产管理、交通运输等诸多方面得到广泛应用，并已发展成为当今应用数学领域一门十分活跃的学科。</p>

<p>最优化问题的一般形式为：</p>

$$
\begin{aligned} &\min \ f\left ( \boldsymbol{x} \right ) \\
&\ \mathrm{s.t.} \ \ \boldsymbol{x} \in X \subseteq R^n \end{aligned}
$$

<p class="post-text-noindent">其中 $x$ 为<strong>决策变量</strong> (decision variable)，$f\left(x \right)$ 为<strong>目标函数</strong> (objective function)，$X$ 为<strong>约束集</strong> (constraint set) 或<strong>可行域</strong> (feasible region)。当 $X = R^n$ 时，称为<strong>无约束优化</strong> (unconstrained optimization) 问题 ，否则称为<strong>约束优化</strong> (constrained optimization) 问题。约束优化问题通常写为如下更具体的形式：</p>

$$
\begin{aligned} &\min \ f\left ( \boldsymbol{x} \right ) \\
&\ \mathrm{s.t.} \ \ c_i\left(\boldsymbol{x}\right) = 0, i \in E \\
&\qquad \ c_i\left(\boldsymbol{x}\right) \geq 0, i \in I \end{aligned}
$$

<p class="post-text-noindent">$c_i\left(\boldsymbol{x}\right) = 0, i \in E$ 为<strong>等式约束</strong> (equality constraint)，$c_i\left(\boldsymbol{x}\right) \geq 0, i \in I$ 为<strong>不等式约束</strong> (inequality constraint)，$c_i\left(\boldsymbol{x}\right)$ 为<strong>约束函数</strong> (constraint function)，$E$ 和 $I$ 分别是等式约束的指标集和不等式约束的指标集。当目标函数与约束函数均为线性函数时，约束优化问题称为<strong>线性规划</strong> (linear programming)，否则称为<strong>非线性规划</strong> (nonlinear programming)。</p>

<p>本章，我们将主要介绍一些数学基础知识，为后续系统学习最优化算法打下坚实的基础，此外，我们还会对最优化算法的基本结构做个简要描述。现在，就让我们放下对数学符号的恐惧，拿起笔和纸，一起在属于 x, y 和 z 的王国里开始遨游吧。学习从来都是痛苦的过程，只有那些不惧艰险、勇于攀登的人，才能最终品尝到属于他们的、独一无二的、最甜也最美的果实。</p>

<h1>1.1 线性代数</h1>

<p>本节，我们介绍最优化理论中需要用到的线性代数知识，包括：范数、矩阵的逆与广义逆、矩阵的 Rayleigh 商和矩阵的秩一校正。</p>

<h2>1.1.1 范数</h2>

<p>范数是长度概念的推广，向量、矩阵均有范数。$R^n$ 上的<strong>向量范数</strong> (vector norm) 是一个从 $R^n \rightarrow R$ 的映射 $\left \| \cdot  \right \|$，它满足如下三个性质：</p>

<ul>
	<li>非负性 (Positivity)：$\left \| \boldsymbol{x}  \right \| \geq 0,\ \forall \ \boldsymbol{x} \in R^n,\ \left \| \boldsymbol{x}  \right \| = 0 \Leftrightarrow \boldsymbol{x} = 0$</li>
	<li>齐次性 (Homogeneity)：$\left \| \alpha \boldsymbol{x}  \right \| =\left | \alpha \right |\left \| \boldsymbol{x} \right \|,\ \forall \alpha \in R, \ \boldsymbol{x} \in R^n$</li>
	<li>三角不等式 (Triangle inequality)：$\left \| \boldsymbol{x}+\boldsymbol{y}  \right \| \leq \left \| \boldsymbol{x} \right \|+\left \| \boldsymbol{y} \right \|,\ \forall \ \boldsymbol{x},\boldsymbol{y} \in R^n$</li>
</ul>

<p class="post-text-noindent">向量 $\boldsymbol{x} = \left(x_1,x_2,\cdots,x_n\right)'$ 的 <strong>$l_p$ 范数</strong>定义为：</p>

$$\left \| \boldsymbol{x}  \right \|_p = \left ( \sum_{i=1}^{n}\left | x_i \right |^p \right )^{\frac{1}{p}},\ 1 \leq p < \infty$$

<p class="post-text-noindent">常用的向量范数如下所示：</p>

<ul>
	<li><strong>$l_1$ 范数</strong> $\left ( l_1\ \,\,\mathrm{norm} \right )$:</li>
	$$\left \| \boldsymbol{x}  \right \|_1 = \sum_{i=1}^{n}\left | x_i \right |$$
	<li><strong>$l_2$ 范数</strong> $\left ( l_2\ \,\,\mathrm{norm} \right )$:</li>
	$$\left \| \boldsymbol{x}  \right \|_2 = \left (\sum_{i=1}^{n} x_i^2 \right )^{\frac{1}{2}}$$
	<li><strong>$l_\infty$ 范数</strong> $\left ( l_\infty\  \mathrm{norm} \right )$:</li>
	$$\left \| \boldsymbol{x}  \right \|_\infty = \max_{1\leq i\leq n}\left | x_i \right |$$
	<li><strong>椭球范数</strong> $\left(\mathrm{ellipsoidal} \ \mathrm{norm}\right)$:</li>
	$$\left \| \boldsymbol{x}  \right \|_{\boldsymbol{A}} = \left ( x^T\boldsymbol{A}x \right )^{\frac{1}{2}},\boldsymbol{A}^T=\boldsymbol{A},\boldsymbol{A}_{n\times n} > 0$$
</ul>

<p class="post-text-noindent">上述四个向量范数是等价的，这是因为它们满足如下四个不等式：</p>

$$\left \| \boldsymbol{x} \right \|_2\ \ \leq \left \| \boldsymbol{x} \right \|_1 \leq \sqrt{n}\left \| \boldsymbol{x} \right \|_2$$
$$\left \| \boldsymbol{x} \right \|_\infty\  \leq \left \| \boldsymbol{x} \right \|_2 \leq \sqrt{n}\left \| \boldsymbol{x} \right \|_\infty$$
$$\left \| \boldsymbol{x} \right \|_\infty\  \leq \left \| \boldsymbol{x} \right \|_1 \leq n\left \| \boldsymbol{x} \right \|_\infty$$
$$\sqrt{\lambda_{\mathrm{min}}\left ( \boldsymbol{A} \right )}\left \| \boldsymbol{x} \right \|_2\ \  \leq \left \| \boldsymbol{x} \right \|_{\boldsymbol{A}} \leq \sqrt{\lambda_{\mathrm{max}}\left ( \boldsymbol{A} \right )}\left \| \boldsymbol{x} \right \|_2$$

<p class="post-text-noindent">其中 $\lambda_{\mathrm{max}} \left ( \boldsymbol{A} \right )$ 为矩阵 $\boldsymbol{A}$ 的最大特征值，$\lambda_{\mathrm{min}} \left ( \boldsymbol{A} \right )$ 为矩阵 $\boldsymbol{A}$ 的最小特征值。</p>

<blockquote>等价范数：如果 $\exists \mu_1,\ \mu_2>0$ 使得 $R^n$ 上的范数 $\left \| \cdot  \right \|_\alpha$ 和 $\left \| \cdot  \right \|_\beta$ 满足：$\mu_1\left \| \boldsymbol{x} \right \|_\alpha \leq \left \| \boldsymbol{x} \right \|_\beta \leq \mu_2\left \| \boldsymbol{x} \right \|_\alpha$，$\forall \ \boldsymbol{x}\in R^n$，则我们称 $R^n$ 上的范数 $\left \| \cdot  \right \|_\alpha$ 和 $\left \| \cdot  \right \|_\beta$ 是等价的。</blockquote>

<p class="post-text-noindent">此外，关于向量范数，还有几个重要的不等式：</p>

<ul>
	<li>$\left | \boldsymbol{x}^T\boldsymbol{A}\boldsymbol{y} \right |\leq\left \| \boldsymbol{x} \right \|_{\boldsymbol{A}}\left \| \boldsymbol{y} \right \|_{\boldsymbol{A}},\boldsymbol{A}_{n\times n}>0;\ \left | \boldsymbol{x}^T\boldsymbol{A}\boldsymbol{y} \right |=\left \| \boldsymbol{x} \right \|_{\boldsymbol{A}}\left \| \boldsymbol{y} \right \|_{\boldsymbol{A}} \Leftrightarrow \boldsymbol{x}=c\boldsymbol{y},c\neq 0$（<span class="post-text-problem">待证明</span>）</li>
	<li>$\left | \boldsymbol{x}^T \boldsymbol{y} \right |\leq\left \| \boldsymbol{x} \right \|_{\boldsymbol{A}}\left \| \boldsymbol{y} \right \|_{\boldsymbol{A}^{-1}},\boldsymbol{A}_{n\times n}>0;\ \left | \boldsymbol{x}^T\boldsymbol{y} \right |=\left \| \boldsymbol{x} \right \|_{\boldsymbol{A}}\left \| \boldsymbol{y} \right \|_{\boldsymbol{A^{-1}}} \Leftrightarrow \boldsymbol{x}=c\boldsymbol{A^{-1}y},c\neq 0$（<span class="post-text-problem">待证明</span>）</li>
	<li><strong>Young 不等式</strong>：</li>
	$$xy \leq \frac{x^p}{p}+\frac{y^q}{q},\ x,y \geq 0,\ \frac{1}{p}+\frac{1}{q}=1,\ p,q>1;\ xy = \frac{x^p}{p}+\frac{y^q}{q}\Leftrightarrow x^p=y^q$$
</ul>

<blockquote>证明：当 $x = 0$ 或 $y = 0$ 时，显然成立；当 $x,y > 0$ 时，令 $t = \frac{1}{p}$、$1-t = \frac{1}{q}$、$a = x^p$、$b = y^q$，因为 $\ln \left(x \right)$ 是一个凹函数，所以 $\ln \left[ta + \left(1-t \right)b \right] \geq t\ln a + \left(1-t \right)\ln b$，代入 $t,\ 1-t,\ a,\ b$，然后两边同取指数运算，即得上式。</blockquote>

<ul>
	<li><strong>Holder 不等式</strong>（特例：<strong>Cauchy-Schwarz 不等式</strong>）：</li>
	$$\left | \boldsymbol{x}^T \boldsymbol{y} \right |\leq \left \| \boldsymbol{x} \right \|_p\left \| \boldsymbol{y} \right \|_q,\ \frac{1}{p}+\frac{1}{q}=1,\ p,q>1$$
</ul>

<blockquote>证明：由 Young 不等式有</blockquote>

<blockquote>$$\frac{\left | x_iy_i \right |}{\left \| x \right \|_p\left \| y \right \|_q} \leq \frac{1}{p}\left ( \frac{\left | x_i \right |}{\left \| x \right \|_p} \right )^p + \frac{1}{q}\left ( \frac{\left | x_i \right |}{\left \| x \right \|_q} \right )^q$$</blockquote>

<blockquote>上述不等式两边关于 $i$ 求和得</blockquote>

<blockquote>$$\frac{1}{\left \| x \right \|_p\left \| y \right \|_q}\sum_{i=1}^{n}\left | x_iy_i \right | \leq \frac{1}{p\left \| x \right \|_p^p}\sum_{i=1}^{n}\left | x_i \right |^p + \frac{1}{q\left \| y \right \|_q^q}\sum_{i=1}^{n}\left | y_i \right |^q = \frac{1}{p} + \frac{1}{q} = 1$$</blockquote>

<blockquote>两边同乘 $\left \| x \right \|_p\left \| y \right \|_q$ 即得结果。</blockquote>

<ul>
	<li><strong>Minkowski 不等式</strong>（范数定义中的第 3 条性质）：</li>
	$$\left \| \boldsymbol{x}+\boldsymbol{y} \right \|_p\leq \left \| \boldsymbol{x} \right \|_p+\left \| \boldsymbol{y} \right \|_p,\ p \geq 1$$
</ul>

<blockquote>证明：当 $x = 0$ 或 $y = 0$ 时，显然成立；当 $x,y > 0$ 时，令 $t = \frac{\left \| x \right \|_p}{\left \| x  \right \|_p + \left \| y  \right \|_p}$、$1-t = \frac{\left \| y \right \|_p}{\left \| x  \right \|_p + \left \| y  \right \|_p}$、$a = \frac{\left | x_i \right |}{\left \| x \right \|_p}$、$b = \frac{\left | y_i \right |}{\left \| y \right \|_p}$。因为 $x^p,\ x>0$ 是凸函数，所以有 $\left[ta + \left(1-t \right)b \right]^p \leq t a^p + \left(1-t \right) b^p$，代入 $t,\ 1-t,\ a,\ b$，然后两边同时对 $i$ 求和，可得</blockquote>

<blockquote>$$\sum_{i=1}^{n}\left ( \frac{\left | x_i \right |+\left | y_i \right |}{\left \| x \right \|_p + \left \| y \right \|_p} \right )^p \leq 1$$</blockquote>

<blockquote>所以</blockquote>

<blockquote>$$\sum_{i=1}^{n}\left ( \frac{\left | x_i + y_i \right |}{\left \| x \right \|_p + \left \| y \right \|_p} \right )^p \leq \sum_{i=1}^{n}\left ( \frac{\left | x_i \right |+\left | y_i \right |}{\left \| x \right \|_p + \left \| y \right \|_p} \right )^p \leq 1$$</blockquote>

<blockquote>不等号两边同取 $p$ 次根，然后经恒等变换即得结果。</blockquote>

<p>矩阵范数是向量范数的自然推广，$R^{m\times n}$ 上的矩阵可视为 $R^{mn}$ 中的向量。$R^{m\times n}$ 上的<strong>矩阵范数</strong> (matrix norm) 是一个从 $R^{mn} \rightarrow R$ 的映射 $\left \| \cdot  \right \|$，它满足如下三个性质：</p>

<ul>
	<li>非负性：$\left \| \boldsymbol{A}  \right \| \geq 0,\ \forall \ \boldsymbol{A} \in R^{m\times n},\ \left \| \boldsymbol{A}  \right \| = 0 \Leftrightarrow \boldsymbol{A} = \boldsymbol{O}$，$\boldsymbol{O}$ 为一个零矩阵</li>
	<li>齐次性：$\left \| \alpha \boldsymbol{A}  \right \| =\left | \alpha \right |\left \| \boldsymbol{A} \right \|,\ \forall \alpha \in R, \ \boldsymbol{A} \in R^{m\times n}$</li>
	<li>三角不等式：$\left \| \boldsymbol{A}+\boldsymbol{B}  \right \| \leq \left \| \boldsymbol{A} \right \|+\left \| \boldsymbol{B} \right \|,\ \forall \ \boldsymbol{A},\boldsymbol{B} \in R^{m\times n}$</li>
</ul>

<p class="post-text-noindent">如果 $\forall \boldsymbol{A} \in R^{m\times n},\ \boldsymbol{x} \in R^n$ 有：</p>

$$\left \| \boldsymbol{Ax}  \right \| \leq \left \| \boldsymbol{A}  \right \|\left \| \boldsymbol{x}  \right \|$$

<p class="post-text-noindent">我们称该矩阵范数可由向量范数导出，或与向量范数兼容，<strong>诱导 (矩阵) 范数</strong>（induced norm）因此定义为（<span class="post-text-problem">为什么 $\begin{aligned}\left \| \boldsymbol{A^{-1}}  \right \| =1/ \min_{\left \| \boldsymbol{x} \right \|=1}\left \| \boldsymbol{Ax}  \right \|\end{aligned}$</span>）：</p>

$$\left \| \boldsymbol{A}  \right \| =\max_{\boldsymbol{x}\neq 0} \frac{\left \| \boldsymbol{Ax}  \right \|}{ \left \| \boldsymbol{x}  \right \|} = \max_{\left \| \boldsymbol{x} \right \|=1}\left \| \boldsymbol{Ax}  \right \|$$

<blockquote>显然，上式给出的诱导范数的定义满足条件 $\left \| \boldsymbol{Ax}  \right \| \leq \left \| \boldsymbol{A}  \right \|\left \| \boldsymbol{x}  \right \|$，但要保证上式定义的合理性，$f\left( \boldsymbol{x} \right) = \left \| \boldsymbol{Ax}  \right \|$ 在 $D = \left \{\boldsymbol{x} \in R^n: \left \| \boldsymbol{x}  \right \| = 1 \right \}$ 上必须存在最大值。根据向量范数的连续性，以及有界闭集上的连续函数必有最大最小值的定理，我们可以知道上述定义是合理的。</blockquote>

<p class="post-text-noindent">如果对 $n\times n$ 正交矩阵 $\boldsymbol{U}$ 有 $\left \| \boldsymbol{UA}  \right \| = \left \| \boldsymbol{A}  \right \|$，则称 $\left \| \cdot  \right \|$ 为<strong>正交不变范数</strong>。常用的矩阵范数如下所示：</p>

<ul>
	<li><strong>$l_1$ 诱导范数</strong> / <strong>列和范数</strong> $\left ( l_1\ \mathrm{induced\ norm} \right )$:</li>
	$$\left \| \boldsymbol{A}  \right \|_1 = \max_{j}\left \| \boldsymbol{a}_{j} \right \|_1 = \max_{j}\sum_{i=1}^{n}\left | a_{ij} \right |$$
	<li><strong>$l_2$ 诱导范数</strong> / <strong>谱范数</strong> $\left ( l_2\ \mathrm{induced\ norm \ / \ spectral \ norm} \right )$:</li>
	$$\left \| \boldsymbol{A}  \right \|_2 = \sqrt{\lambda_{\mathrm{max}} \left ( \boldsymbol{A}^T\boldsymbol{A} \right )}$$
	<li><strong>$l_\infty$ 诱导范数</strong> / <strong>行和范数</strong> $\left ( l_\infty\ \mathrm{induced\ norm} \right )$:</li>
	$$\left \| \boldsymbol{A}  \right \|_\infty = \max_{i}\left \| \boldsymbol{a}_{i} \right \|_1 = \max_{i}\sum_{j=1}^{n}\left | a_{ij} \right |$$
	<li><strong>Frobenius 范数</strong> $\left (\mathrm{Frobenius\ norm} \right )$:</li>
	$$\left \| \boldsymbol{A}  \right \|_F = \left ( \sum_{i=1}^{n}\sum_{j=1}^{n}\left | a_{ij} \right |^2 \right )^{\frac{1}{2}} = \sqrt{\mathrm{tr}\left ( \boldsymbol{A}^T\boldsymbol{A} \right )}$$
	<li>这里 $\boldsymbol{a}_{j}$ 为矩阵 $\boldsymbol{A}$ 的第 $j$ 列构成的向量，$\boldsymbol{a}_{i}$ 为矩阵 $\boldsymbol{A}$ 的第 $i$ 行构成的向量，$\lambda_{\mathrm{max}} \left ( \boldsymbol{A}^T\boldsymbol{A} \right )$ 为矩阵 $\boldsymbol{A}^T\boldsymbol{A}$ 的最大特征值，$\mathrm{tr}\left ( \boldsymbol{A}^T\boldsymbol{A} \right )$ 为矩阵 $\boldsymbol{A}^T\boldsymbol{A}$ 的迹（主对角线上元素的和）</li>
</ul>

<p class="post-text-noindent">上述常用矩阵范数中，谱范数和 Frobenius 范数为正交不变范数。此外，上述常用矩阵范数均满足<strong>相容性条件</strong>：</p>

$$\left \| \boldsymbol{AB}  \right \| \leq \left \| \boldsymbol{A}  \right \|\left \| \boldsymbol{B}  \right \|,\ \forall \ \boldsymbol{A},\boldsymbol{B} \in R^{n\times n}$$

<p class="post-text-noindent">且有（<span class="post-text-problem">为什么成立</span>）：</p>

$$\left \| \boldsymbol{AB}  \right \|_F \leq \min \left \{ \left \| \boldsymbol{A}  \right \|_2 \left \| \boldsymbol{B}  \right \|_F,\left \| \boldsymbol{A}  \right \|_F \left \| \boldsymbol{B}  \right \|_2 \right \},\ \forall \ \boldsymbol{A},\boldsymbol{B} \in R^{n\times n}$$

<p class="post-text-noindent">在本节最后，我们给出 $l_1$ 诱导范数、$l_2$ 诱导范数和 $l_\infty$ 诱导范数的证明过程</p>

<br>

<blockquote><strong>$l_1$ 诱导范数的证明如下：</strong></blockquote>

<blockquote>$$\begin{aligned}
&\because \ \left \| \boldsymbol{Ax}  \right \|_1=\sum_{i=1}^{m}\left | \sum_{j=1}^{n}a_{ij}x_j \right | \leq \sum_{i=1}^{m}\sum_{j=1}^{n}\left | a_{ij} \right |\left | x_j \right |= \sum_{j=1}^{n}\sum_{i=1}^{m}\left | a_{ij} \right |\left | x_j \right |  \\
&\leq \left ( \max_j \sum_{i=1}^{m}\left | a_{ij} \right | \right )\sum_{j=1}^{n}\left | x_j \right |=\max_j \sum_{i=1}^{m}\left | a_{ij} \right |\left \| \boldsymbol{x} \right \|_1
\end{aligned}$$</blockquote>

<blockquote>$$\therefore \ \left \| \boldsymbol{A}  \right \|_1 =\max_{\boldsymbol{x}\neq 0} \frac{\left \| \boldsymbol{Ax}  \right \|_1}{ \left \| \boldsymbol{x}  \right \|_1} = \max_{\left \| \boldsymbol{x} \right \|_1=1}\left \| \boldsymbol{Ax}  \right \|_1 \leq \max_{j}\sum_{i=1}^{m}\left | a_{ij} \right | \qquad \cdots \left(1\right)$$</blockquote>

<blockquote>取 $\boldsymbol{x^{(j)}}=(0,\cdots,1,\cdots,0),\ j=1,2,\cdots,n$，它是除第 $j$ 个元素为 $1$、其余元素全为 $0$ 的向量，则有</blockquote>

<blockquote>$$\left \| \boldsymbol{A}  \right \|_1 =\max_{\boldsymbol{x}\neq 0} \frac{\left \| \boldsymbol{Ax}  \right \|_1}{ \left \| \boldsymbol{x}  \right \|_1} = \max_{\left \| \boldsymbol{x} \right \|_1=1}\left \| \boldsymbol{Ax}  \right \|_1 \geq \max_{\boldsymbol{x}^{(j)}}\left \| \boldsymbol{Ax}  \right \|_1=\max_{j}\sum_{i=1}^{m}\left | a_{ij} \right | \qquad \cdots \left(2\right)$$</blockquote>

<blockquote>由 $\left(1 \right)$、$\left(2 \right)$ 两式知</blockquote>

<blockquote>$$\left \| \boldsymbol{A}  \right \|_1 =\max_{j}\sum_{i=1}^{m}\left | a_{ij} \right |$$</blockquote>

<br>

<blockquote><strong>$l_2$ 诱导范数的证明如下：</strong></blockquote>

<blockquote>设 $\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_n$ 为对称半正定矩阵 $A^TA$ 的与特征值 $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n$ 相对应的相互正交的特征向量，则 $\forall \ \boldsymbol{x} = 1$ 的向量 $\boldsymbol{x}$，必存在满足条件 $c_1^2+c_2^2+\cdots+c_n^2=1$ 的 $c_1,c_2,\cdots,c_n$ 使得 $\boldsymbol{x}=c_1\boldsymbol{x}_1+c_2\boldsymbol{x}_2+\cdots+c_n\boldsymbol{x}_n$。所以有：</blockquote>

<blockquote>$$\begin{aligned}
&\ \left \| \boldsymbol{Ax} \right \|_2^2 = \left ( \boldsymbol{Ax} \right )^T \boldsymbol{Ax} = \boldsymbol{x}^T\boldsymbol{A}^T\boldsymbol{A}\boldsymbol{x} \\
&= \left ( c_1\boldsymbol{x}_1+c_2\boldsymbol{x}_2+\cdots+c_n\boldsymbol{x}_n \right )^T\boldsymbol{A}^T\boldsymbol{A}\left ( c_1\boldsymbol{x}_1+c_2\boldsymbol{x}_2+\cdots+c_n\boldsymbol{x}_n \right ) \\
&= \left ( c_1\boldsymbol{x}_1+c_2\boldsymbol{x}_2+\cdots+c_n\boldsymbol{x}_n \right )^T\left ( c_1\boldsymbol{A}^T\boldsymbol{A}\boldsymbol{x}_1+c_2\boldsymbol{A}^T\boldsymbol{A}\boldsymbol{x}_2+\cdots+c_n\boldsymbol{A}^T\boldsymbol{A}\boldsymbol{x}_n \right ) \\
&= \left ( c_1\boldsymbol{x}_1+c_2\boldsymbol{x}_2+\cdots+c_n\boldsymbol{x}_n \right )^T\left ( c_1\lambda_1\boldsymbol{x}_1+c_2\lambda_2\boldsymbol{x}_2+\cdots+c_n\lambda_n\boldsymbol{x}_n \right )\\
&= \left ( c_1\boldsymbol{x}_1^T+c_2\boldsymbol{x}_2^T+\cdots+c_n\boldsymbol{x}_n^T \right )\left ( c_1\lambda_1\boldsymbol{x}_1+c_2\lambda_2\boldsymbol{x}_2+\cdots+c_n\lambda_n\boldsymbol{x}_n \right ) \\
&= \sum_{i=1}^n\sum_{j=1}^n\lambda_i\lambda_j c_ic_j \boldsymbol{x}_i^T\boldsymbol{x}_j = \sum_{i=1}^n \lambda_i c_i^2 \\
&\leq \lambda_1 \sum_{i=1}^n c_i^2 = \lambda_1 \qquad \cdots \left(3\right)
\end{aligned}$$</blockquote>

<blockquote>又因为 $\left \| \boldsymbol{x}_1 \right \|_2 = 1$ 且：</blockquote>

<blockquote>$$\left \| \boldsymbol{Ax}_1 \right \|_2^2 = \boldsymbol{x}_1^T\boldsymbol{A}^T\boldsymbol{A}\boldsymbol{x}_1 =  \boldsymbol{x}_1^T\lambda_1\boldsymbol{x}_1 = \lambda_1 \qquad \cdots \left(4\right)$$</blockquote>

<blockquote>所以由 $\left(3\right)$、$\left(4\right)$ 式得：</blockquote>

<blockquote>$$\left \| \boldsymbol{A}  \right \|_2 =\max_{\boldsymbol{x}\neq 0} \frac{\left \| \boldsymbol{Ax}  \right \|_2}{ \left \| \boldsymbol{x}  \right \|_2} = \max_{\left \| \boldsymbol{x} \right \|_2=1}\left \| \boldsymbol{Ax}  \right \|_2 = \sqrt{\lambda_1}$$</blockquote>

<br>

<blockquote><strong>$l_\infty$ 诱导范数的证明如下：</strong></blockquote>

<blockquote>$$\begin{aligned}
&\because \ \left \| \boldsymbol{Ax}  \right \|_\infty=\max_{1\leq i \leq m}\left | \sum_{j=1}^{n}a_{ij}x_j \right | \leq \max_{1\leq i \leq m}\sum_{j=1}^{n}\left | a_{ij} \right |\left | x_j \right | \\
&\leq \max_{1\leq i \leq m} \left ( \max_{1\leq j \leq n}\left | x_j \right | \right ) \sum_{i=1}^{m}\left | a_{ij} \right | =\max_i \sum_{i=1}^{m}\left | a_{ij} \right |\left \| \boldsymbol{x} \right \|_\infty
\end{aligned}$$</blockquote>

<blockquote>$$\therefore \ \left \| \boldsymbol{A}  \right \|_\infty =\max_{\boldsymbol{x}\neq 0} \frac{\left \| \boldsymbol{Ax}  \right \|_\infty}{ \left \| \boldsymbol{x}  \right \|_\infty} = \max_{\left \| \boldsymbol{x} \right \|_\infty=1}\left \| \boldsymbol{Ax}  \right \|_\infty \leq \max_{i}\sum_{i=1}^{n}\left | a_{ij} \right | \qquad \cdots \left(5\right)$$</blockquote>

<blockquote>取 $k$ 使得</blockquote>

<blockquote>$$\sum_{j=1}^{n}\left | a_{kj} \right | = \max_{i}\sum_{j=1}^{n}\left | a_{ij} \right |$$</blockquote>

<blockquote>令 $\boldsymbol{y} = \left(y_1,y_2,\cdots,y_n \right)^T$，其中</blockquote>

<blockquote>$$\ y_j = \left\{\begin{matrix}
\begin{aligned}
&\frac{\left | a_{kj} \right |}{a_{kj}},\ a_{kj} \neq 0
\\ &1\ \ \ \ \ \ \, , \ otherwise
\end{aligned}
\end{matrix}\right.$$</blockquote>

<blockquote>易知 $\left \| \boldsymbol{y} \right \|_\infty = 1$，从而有</blockquote>

<blockquote>$$\begin{aligned}
&\left \| \boldsymbol{A}  \right \|_\infty =\max_{\boldsymbol{x}\neq 0} \frac{\left \| \boldsymbol{Ax}  \right \|_\infty}{ \left \| \boldsymbol{x}  \right \|_\infty} = \max_{\left \| \boldsymbol{x} \right \|_\infty=1}\left \| \boldsymbol{Ax}  \right \|_\infty \geq \left \| \boldsymbol{Ay} \right \|_\infty = \max_{1 \leq i \leq m}\left | \sum_{j=1}^{n}a_{ij}y_j \right |\\
&\geq \left | \sum_{j=1}^{n}a_{kj}y_j \right | = \sum_{j=1}^{n}\left | a_{kj} \right |= \max_{i}\sum_{j=1}^{n}\left | a_{ij} \right |\ \cdots \left(6\right)
\end{aligned}$$</blockquote>

<blockquote>由 $\left(5 \right)$、$\left(6 \right)$ 两式知：</blockquote>

<blockquote>$$\left \| \boldsymbol{A}  \right \|_\infty =\max_{i}\sum_{j=1}^{n}\left | a_{ij} \right |$$</blockquote>

<h2>1.1.2 矩阵的逆与广义逆</h2>

<p>关于矩阵的逆，有一个重要的结论，那就是：<strong>接近于可逆矩阵的矩阵可逆</strong>，其接近度由两个矩阵差的范数衡量。下面是这一结论的严格数学表述：</p>

<blockquote><strong>定理 1</strong>：设 $\boldsymbol{A},\boldsymbol{B} \in R^{n\times n}$，$\boldsymbol{A}$ 可逆，$\left \| \boldsymbol{A}^{-1} \right \| \leq \alpha$，如果 $\left \| \boldsymbol{A}-\boldsymbol{B} \right \| \leq \beta$，$\alpha \beta < 1$（注：矩阵范数为相容矩阵范数），则 $\boldsymbol{B}$ 可逆且</blockquote>

<blockquote>$$\left \| \boldsymbol{B}^{-1} \right \| \leq \frac{\alpha}{1-\alpha \beta}$$</blockquote>

<p class="post-text-noindent">下面是定理 1 的证明过程：</p>

<blockquote><strong>证明</strong>：令 $\boldsymbol{E} = \boldsymbol{A}^{-1}\left(\boldsymbol{A}-\boldsymbol{B} \right) = \boldsymbol{I} - \boldsymbol{A}^{-1}\boldsymbol{B}$，因为</blockquote>

<blockquote>$$\left \| \boldsymbol{E} \right \| = \left \| \boldsymbol{A}^{-1}\left(\boldsymbol{A}-\boldsymbol{B} \right) \right \| \leq \left \| \boldsymbol{A}^{-1} \right \|\left \| \boldsymbol{A}-\boldsymbol{B} \right \| \leq \alpha \beta < 1$$</blockquote>

<blockquote>所以 $\sum_{k=0}^{\infty} \boldsymbol{E}^k$ 存在，且容易验证：</blockquote>

<blockquote>$$\sum_{k=0}^{\infty} \boldsymbol{E}^k = \left ( \boldsymbol{I}-\boldsymbol{E} \right )^{-1}$$</blockquote>

<blockquote>从而</blockquote>

<blockquote>$$\left \| \left (\boldsymbol{I}-\boldsymbol{E}  \right )^{-1} \right \| \leq \sum_{k=0}^{\infty}\left \| \boldsymbol{E} \right \|^k = \frac{1}{1-\left \| \boldsymbol{E} \right \|}$$</blockquote>

<blockquote>代入 $\boldsymbol{E}$，得：</blockquote>

<blockquote>$$\left \| \boldsymbol{B}^{-1} \right \| \leq \left \| \boldsymbol{B}^{-1}\boldsymbol{A} \right \|\left \| \boldsymbol{A}^{-1} \right \| \leq \frac{\left \| \boldsymbol{A}^{-1} \right \|}{1-\left \| \boldsymbol{A}^{-1}\left(\boldsymbol{A}-\boldsymbol{B} \right) \right \|} \leq \frac{\left \| \boldsymbol{A}^{-1} \right \|}{1-\left \| \boldsymbol{A}^{-1} \right \|\left \| \boldsymbol{A}-\boldsymbol{B} \right \|} \leq \frac{\alpha}{1-\alpha \beta}$$</blockquote>

<p>设 $\boldsymbol{A}$ 为 $m\times n$ 复矩阵，则矩阵方程 $\boldsymbol{A}\boldsymbol{X}\boldsymbol{A} = \boldsymbol{A}$ 的每一个解称为 $\boldsymbol{A}$ 的<strong>广义逆</strong>，记作 $\boldsymbol{A}^{-}$（注：可以证明该矩阵方程一定有解，证明过程可参见丘维声所著 《高等代数(第二版)》 上册第 5 章第 3 节的定理 1）。为了避免上述矩阵方程解的不唯一性，即广义逆的不唯一性，我们引入 Penrose 方程组（注：$\boldsymbol{A}^{*}$ 表示 $\boldsymbol{A}$ 的共轭转置）</p>

$$\left\{\begin{matrix}
\boldsymbol{AXA} \, = \boldsymbol{A} \ \ \\
\boldsymbol{XAX} = \boldsymbol{X} \ \\
\left(\boldsymbol{AX} \right)^{*} = \boldsymbol{AX} \\
\left(\boldsymbol{XA} \right)^{*} = \boldsymbol{XA}
\end{matrix}\right.$$

<p class="post-text-noindent">我们将上述方程组的解称为 $\boldsymbol{A}$ 的 <strong>Moore-Penrose 广义逆</strong>，记作 $\boldsymbol{A}^{+}$。下面给出与 Moore-Penrose 广义逆有关的两个正交投影算子：</p>

<blockquote><strong>定义 1</strong>：设 $\mathcal{V}$ 为 $R^n$ 的子空间, $\mathcal{V}^{\perp}$ 为子空间的<strong>正交补</strong>，即 $\mathcal{V}^{\perp} = \left \{ \boldsymbol{x}:\boldsymbol{v}^{*}\boldsymbol{x}=0,\forall \ \boldsymbol{v}\in \mathcal{V} \right \}$，如果线性算子 $P$ 满足：$\forall \boldsymbol{y} \in \mathcal{V},\ P\boldsymbol{y} = \boldsymbol{y}$，$\forall \boldsymbol{z} \in \mathcal{V
^{\perp}},\ P\boldsymbol{z} = 0$，那么我们称 $P$ 是从 $R^n$ 沿子空间 $\mathcal{V}^{\perp}$ 到子空间 $\mathcal{V}$ 的<strong>正交投影算子</strong>。</blockquote>

<blockquote><strong>定理 2</strong>：$\boldsymbol{A}\boldsymbol{A}^{+}$ 是从 $R^n$ 沿 $\boldsymbol{A}^{*}$ 零空间 $\mathcal{N}\left(\boldsymbol{A}^{*} \right)$ 到 $\boldsymbol{A}$ 的象空间 $\mathcal{R}\left(\boldsymbol{A} \right)$ 的正交投影算子，$\boldsymbol{A}^{+}\boldsymbol{A}$ 是从 $R^n$ 沿 ${\boldsymbol{A}^{+}}^{*}$ 的零空间 $\mathcal{N}\left({\boldsymbol{A}^{+}}^{*} \right)$ 到 $\boldsymbol{A}^{+}$ 的象空间 $\mathcal{R}\left(\boldsymbol{A}^{+} \right)$ 的正交投影算子。其中 $\mathcal{R} \left(\boldsymbol{A} \right) \triangleq \left \{ \boldsymbol{Ax}: \boldsymbol{x} \in R^n\right \}$，$\mathcal{N} \left(\boldsymbol{A} \right) \triangleq \left \{ \boldsymbol{x} \in R^n:\boldsymbol{Ax}=0\right \}$。</blockquote>

<p class="post-text-noindent">下面是定理 2 的证明过程（我们仅给出第一部分的证明，第二部分证明同理可得）：</p>

<blockquote><strong>证明</strong>：我们首先证明 ${\mathcal{R}\left(\boldsymbol{A} \right)}^{\perp} = \mathcal{N}\left(\boldsymbol{A}^{*} \right)$. </blockquote>

<blockquote>如果 $\boldsymbol{x} \in \mathcal{R}\left(\boldsymbol{A} \right)^{\perp}$，则 $\forall \boldsymbol{y} \in R^n$，有 $\left(\boldsymbol{Ay} \right) \in \mathcal{R}\left(\boldsymbol{A} \right)$，所以由正交补的定义有 $\left(\boldsymbol{Ay} \right)^{*} \boldsymbol{x} = 0$，因此 $\boldsymbol{y}^{*} \left( \boldsymbol{A}^{*} \boldsymbol{x} \right) = 0$，从而有 $\boldsymbol{A}^{*} \boldsymbol{x} = \boldsymbol{0}$，即 $\boldsymbol{x} \in \mathcal{N}\left(\boldsymbol{A}^{*} \right)$，所以 $\mathcal{R}\left(\boldsymbol{A} \right)^{\perp} \subset \mathcal{N}\left(\boldsymbol{A}^{*} \right)$.</blockquote>

<blockquote>如果 $\boldsymbol{x} \in \mathcal{N}\left(\boldsymbol{A}^{*} \right)$，则 $\left(\boldsymbol{A}^{*}\boldsymbol{x} \right) = 0$，$\forall \boldsymbol{y} \in R^n$，有 $\boldsymbol{y}^{*} \left(\boldsymbol{A}^{*}\boldsymbol{x} \right) = \left(\boldsymbol{Ay} \right)^{*} \boldsymbol{x} = 0$，由 $\left(\boldsymbol{Ay} \right)^{*} \boldsymbol{x} = 0$ 知 $\boldsymbol{x} \in \mathcal{R}\left(\boldsymbol{A} \right)^{\perp}$，所以 $\mathcal{N}\left(\boldsymbol{A}^{*} \right) \subset \mathcal{R}\left(\boldsymbol{A} \right)^{\perp}$.</blockquote>

<blockquote>综上，${\mathcal{R}\left(\boldsymbol{A} \right)}^{\perp} = \mathcal{N}\left(\boldsymbol{A}^{*} \right)$。下证 $\boldsymbol{A}\boldsymbol{A}^{+}$ 是从 $R^n$ 沿 $\boldsymbol{A}^{*}$ 零空间 $\mathcal{N}\left(\boldsymbol{A}^{*} \right)$ 到 $\boldsymbol{A}$ 的象空间 $\mathcal{R}\left(\boldsymbol{A} \right)$ 的正交投影算子。</blockquote>

<blockquote>因为 $\forall \boldsymbol{y} \in \mathcal{R} \left(\boldsymbol{A} \right)$, 有 $\boldsymbol{y} = \boldsymbol{Ax}$ . 又由 Moore-Penrose 广义逆的定义有 $\boldsymbol{A}\boldsymbol{A}^{+}\boldsymbol{A} = \boldsymbol{A}$，所以 $\forall \boldsymbol{y} \in \mathcal{R} \left(\boldsymbol{A} \right)$，$\boldsymbol{A}\boldsymbol{A}^{+}\boldsymbol{y} = \boldsymbol{A}\boldsymbol{A}^{+}\boldsymbol{Ax} = \boldsymbol{Ax} = \boldsymbol{y}$；</blockquote>

<blockquote>因为 $\forall \boldsymbol{z} \in \mathcal{N} \left(\boldsymbol{A}^{*} \right)$，有 $\boldsymbol{A}^{*} \boldsymbol{z} = 0$，又由 Moore-Penrose 广义逆的定义有 $\left(\boldsymbol{AA}^{+} \right)^{*} = \left(\boldsymbol{AA}^{+} \right)$，所以 $\forall \boldsymbol{z} \in \mathcal{N} \left(\boldsymbol{A}^{*} \right)$，有 $\boldsymbol{A}\boldsymbol{A}^{+}\boldsymbol{z} = \left(\boldsymbol{A}\boldsymbol{A}^{+} \right )^{*}\boldsymbol{z} = {\boldsymbol{A}^{+}}^{*} \boldsymbol{A}^{*}\boldsymbol{z} = 0$</blockquote>

<blockquote>综上，$\boldsymbol{A}\boldsymbol{A}^{+}$ 是从 $R^n$ 沿 $\boldsymbol{A}^{*}$ 零空间 $\mathcal{N}\left(\boldsymbol{A}^{*} \right)$ 到 $\boldsymbol{A}$ 的象空间 $\mathcal{R}\left(\boldsymbol{A} \right)$ 的正交投影算子。</blockquote>

<p class="post-text-noindent">本节最后，我们给出矩阵正交分解和奇异值分解的 Moore-Penrose 广义逆。</p>

<blockquote><strong>定理 3</strong>：若 $A$ 是秩为 $r$ 的 $m\times n$ 复矩阵，其正交分解为</blockquote>

<blockquote>$$\boldsymbol{A} = \boldsymbol{Q}^*\boldsymbol{RP},\ \boldsymbol{Q}^*\boldsymbol{Q} = \boldsymbol{P}^*\boldsymbol{P} = \boldsymbol{I},\ \boldsymbol{R} = \begin{bmatrix}
\boldsymbol{R}_{r\times r}^{11} & \boldsymbol{O}\\
\boldsymbol{O} & \boldsymbol{O}
\end{bmatrix},\ \left | \boldsymbol{R}_{r\times r}^{11} \right | \neq 0$$</blockquote>

<blockquote>其奇异值分解为</blockquote>

<blockquote>$$\boldsymbol{A} = \boldsymbol{U}\boldsymbol{DV}^*,\ \boldsymbol{U}^*\boldsymbol{U} = \boldsymbol{V}^*\boldsymbol{V} = \boldsymbol{I},\ \boldsymbol{D} = \begin{bmatrix}
\boldsymbol{\Sigma}_{r\times r} & \boldsymbol{O}\\
\boldsymbol{O} & \boldsymbol{O}
\end{bmatrix},\ \boldsymbol{\Sigma}_{r\times r} = \mathrm{diag}\left ( \lambda_1,\cdots,\lambda_r \right )$$</blockquote>

<blockquote>这里 $\boldsymbol{R}_{r\times r}^{11}$ 为上三角矩阵，$\boldsymbol{O}$ 为零矩阵，则有 $\boldsymbol{A}^{+} = \boldsymbol{P}^{*}\boldsymbol{R}^{+}\boldsymbol{Q}$ 或者 $\boldsymbol{A}^{+} = \boldsymbol{V}\boldsymbol{D}^{+}\boldsymbol{U}^{*}$，其中</blockquote>

<blockquote>$$\boldsymbol{R}^{+} = \begin{bmatrix}
{\boldsymbol{R}_{r\times r} ^{11}}^{-1} & \boldsymbol{O}\\
\boldsymbol{O} & \boldsymbol{O}
\end{bmatrix},\ \boldsymbol{D}^{+} = \begin{bmatrix}
\boldsymbol{\Sigma}_{r\times r} ^{-1} & \boldsymbol{O}\\
\boldsymbol{O} & \boldsymbol{O}
\end{bmatrix}$$</blockquote>

<h2>1.1.3 矩阵的 Rayleigh 商</h2>
<h2>1.1.4 矩阵的秩一校正</h2>

<h1>1.2 微积分</h1>

<p>本节，我们介绍最优化理论中需要用到的微积分知识，包括：函数的微分、向量值函数的微分和有限差分导数。</p>

<h2>1.2.1 向量序列的收敛</h2>
<h2>1.2.2 函数的微分</h2>
<h2>1.2.3 向量值函数的微分</h2>
<h2>1.2.4 有限差分导数</h2>

<h1>1.3 凸分析</h1>

<p>本节，我们介绍最优化理论中需要用到的凸分析知识，包括：凸集、凸函数和凸集的分离与支撑。</p>

<h2>1.3.1 凸集</h2>
<h2>1.3.2 凸函数</h2>
<h2>1.3.3 凸集的分离与支撑</h2>

<h1>1.4 最优性条件</h1>

<p>本节，我们介绍最优化理论中目标函数取到极值的条件即最优性条件，包括：无约束问题的最优性条件和约束问题的最优性条件。</p>

<h2>1.4.1 无约束问题的最优性条件</h2>
<h2>1.4.2 约束问题的最优性条件</h2>

<h1>1.5 最优化算法的基本结构</h1>

</div>